

<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-173306653-1"></script> -->
<!-- <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-173306653-1');
</script> -->


<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1000px;
    }

    h1 {
        font-weight:300;
        text-align: center;
    }
    h4 {
        text-align: center;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    hr {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
    
    div {
      text-align: justify;
      text-justify: inter-word;
    }
    .caption {
      font-size: 14px;
      font-style: italic;
      /* text-align: center; */
      text-align: justify;
      text-justify: inter-word;
    }
    .bigvideo {
      /* loop controls muted autoplay */
      width: 1000px;
    }

</style>

<html>
  <head>
        <title>4D Humans</title>
        <meta property="og:title" content="4DH" />
        <!-- <meta property="og:image" content="https://shubham-goel.github.io/ucmr/resources/images/teaser.png" /> -->
        <!-- #TODO -->
        <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=9-Ttb8jsevo" />  -->
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:40px">Humans in 4D: <br> Reconstructing and Tracking Humans with Transformers</span>
    </center>

    <br><br>
      <table align=center width=1000px>
       <tr>
        <center>
          <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~shubham-goel/">Shubham Goel</a></span> &nbsp;&nbsp;&nbsp;&nbsp;
          
          <span style="font-size:20px"><a href="https://geopavlakos.github.io/">Georgios Pavlakos</a></span> &nbsp;&nbsp;&nbsp;&nbsp;
          
          <span style="font-size:20px"><a href="http://people.eecs.berkeley.edu/~jathushan/">Jathushan Rajasegaran</a></span> &nbsp;&nbsp;&nbsp;&nbsp;
              
          <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa<sup>*</sup> </a></span> &nbsp;&nbsp;&nbsp;&nbsp;
          
          
          <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~malik/">Jitendra Malik<sup>*</sup> </a></span>
        </center>
     </tr>
    </table>

    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">University of California, Berkeley</span>
                </center>
            </td>
        </tr>
    </table>

    <!-- <br>
    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">arXiv preprint 2023</span>
                </center>
            </td>
        </tr>
    </table> -->


    <br>
    <table align=center width=400px>
     <tr>
       <td align=center width=100px>
       <center>
       <span style="font-size:20px">
        <a href="https://arxiv.org/pdf/2305.20091.pdf">
          <svg style="width:36px;height:36px" viewBox="0 0 24 24">
            <path fill="currentColor" d="M16 0H8C6.9 0 6 .9 6 2V18C6 19.1 6.9 20 8 20H20C21.1 20 22 19.1 22 18V6L16 0M20 18H8V2H15V7H20V18M4 4V22H20V24H4C2.9 24 2 23.1 2 22V4H4M10 10V12H18V10H10M10 14V16H15V14H10Z" />
          </svg>
          <br>
          Paper
        </a>
      </span>
       </center>
       </td>


       <!-- <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://arxiv.org/abs/2007.10982">[Arxiv]</a></span>
        </center>
        </td> -->

      <!-- <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://youtu.be/9-Ttb8jsevo">[Video]</a></span>
      </center>
      </td> -->

      <td align=center width=100px>
      <center>
      <span style="font-size:20px">
        <a href="https://github.com/shubham-goel/4D-Humans">
          <svg style="width:36px;height:36px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M278.9 511.5l-61-17.7c-6.4-1.8-10-8.5-8.2-14.9L346.2 8.7c1.8-6.4 8.5-10 14.9-8.2l61 17.7c6.4 1.8 10 8.5 8.2 14.9L293.8 503.3c-1.9 6.4-8.5 10.1-14.9 8.2zm-114-112.2l43.5-46.4c4.6-4.9 4.3-12.7-.8-17.2L117 256l90.6-79.7c5.1-4.5 5.5-12.3.8-17.2l-43.5-46.4c-4.5-4.8-12.1-5.1-17-.5L3.8 247.2c-5.1 4.7-5.1 12.8 0 17.5l144.1 135.1c4.9 4.6 12.5 4.4 17-.5zm327.2.6l144.1-135.1c5.1-4.7 5.1-12.8 0-17.5L492.1 112.1c-4.8-4.5-12.4-4.3-17 .5L431.6 159c-4.6 4.9-4.3 12.7.8 17.2L523 256l-90.6 79.7c-5.1 4.5-5.5 12.3-.8 17.2l43.5 46.4c4.5 4.9 12.1 5.1 17 .6z"/></svg>
          <br>
          Code
        </a>
      </span>
      </center>
      </td>

      <td align=center width=100px>
      <center>
      <span style="font-size:20px">
        <a href="https://huggingface.co/spaces/brjathu/HMR2.0">
          <img style="width:36px;height:36px" src = "resources/huggingface_logo-noborder.svg" alt="Huggging Faces Demo"/>
          <br>
          Demo
        </a>
      </span>
      </center>
      </td>

      <td align=center width=100px>
        <center>
        <span style="font-size:20px">
          <a href="https://colab.research.google.com/drive/1Ex4gE5v1bPR3evfhtG7sDHxQGsWwNwby">
            <img style="width:36px;height:36px" src = "resources/Google_Colaboratory_SVG_Logo.svg" alt="Google Colab Demo"/>
            <br>
            Colab
          </a>
        </span>
        </center>
        </td>
  
   </tr>
    </table>

            <br>
            <br>
            <table align=center width=1000px>
                <tr>
                    <td width=1000px>
                      <center>
                          <video loop controls muted autoplay class="bigvideo">
                            <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video1.mp4" type="video/mp4">
                          </video>
                    </center>
                    </td>
                </tr>
                <tr>
                    <td width=200px class="caption">
                          We present an approach that, given any video in the wild, can jointly reconstruct the underlying humans in 3D, and track them over time. 
                            <!-- At the core, is HMR 2.0, our robust model for recovering human meshes from single images.  -->
                            Here, we show the input video on the left, and the reconstructed humans on the right, wihtout any temporal smoothness, with colors indicating track identities over time. Our approach works reliably on usual and unusual poses, under poor visibility, extreme truncations and extreme occlusions.
                    </td>
                </tr>
            </table>

            <br><br>
            <!-- Bold abstract subtitle -->
            <hr>
            <div>
              <h1>Abstract</h1>
              We present an approach to reconstruct humans and track them over time. At the core of our approach, we propose a fully "transformerized" version of a network for human mesh recovery. This network, HMR 2.0, advances the state of the art and shows the capability to analyze unusual poses that have in the past been difficult to reconstruct from single images. To analyze video, we use 3D reconstructions from HMR 2.0 as input to a tracking system that operates in 3D. This enables us to deal with multiple people and maintain identities through occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art results for tracking people from monocular video. Furthermore, we demonstrate the effectiveness of HMR 2.0 on the downstream task of action recognition, achieving significant improvements over previous pose-based action recognition approaches.
            </div>
            <br><br>
            <hr>

            <h1>Approach</h1>
            <table>
                <tr>
                  <img class="round" style="width:1000px" src="./approach.png"/>
                </tr>
                <tr>
                  <td class="caption">
                    Left: HMR 2.0 is a fully "transformerized" version of a network for Human Mesh Recovery, containing a ViT and a cross-attention-based transformer decoder. 
                    Right: We use HMR 2.0 as the backbone of our 4DHumans system, that builds on <a href="https://people.eecs.berkeley.edu/~jathushan/PHALP/">PHALP</a>, to jointly reconstruct and track humans in 4D.
                  </td>
              </tr>
              </table>
              <br>
              <hr>

              <h1>Results</h1>
              <h4>Comparisons to existing approaches</h4>
              <table>
                <tr>
                  <td>
                          <video loop controls muted autoplay class="bigvideo">
                          <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video2_comp.mp4" type="video/mp4">
                          </video>
                  </td>
                </tr>

                <tr>
                    <td class="caption">
                      We present comparisons to previous state-of-the-art approaches (PyMAF-X and PARE) for lifting humans to 3D. HMR 2.0 reconstructions are temporally stable, and are better aligned to the input video. The baselines have a lot of jitter and sometimes even fail completely on hard poses or unusual viewpoints.
                    </td>
                </tr>
                </table>
                <br>

                <center><h4>Side-View Visualizations</h4></center>
                <table align=center>
                  <tr>
                    <td>
                      <center>
                                <video loop controls muted autoplay width="700px">
                                <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video3_side.mp4" type="video/mp4">
                                </video>

                    </center>
                    </td>
                  </tr>
                  <tr>
                      <td class="caption">
                        Here, we visualize the reocnstructed mesh from a novel view, in addition to the camera view. HMR 2.0 reconstructions are plausible and temporally stable, even when observed from a novel view, despite it being per-frame 3D reconstruction without temporal smoothness.
                      </td>
                  </tr>
                </table>
                <br>

                <!-- <center><h3>Amodal Completion</h3></center>
                <table align=center width=900px>
                  <tr>
                    <td width=900px>
                      <center>
                          
                                <video loop controls width="900px">
                                <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video4_amodal.mp4" type="video/mp4">
                                </video>

                    </center>
                    </td>
                  </tr>
                  <tr>
                      <td width=600px>
                        <center>
                            <span style="font-size:14px"><i>We can amodally complete our tracklets in frames with missing detections (e.g., occlusions)</i>
                      </center>
                      </td>
                  </tr>
                </table>
                <br> -->
                <hr>


            <table align=center width=1000px>
              <tr>
                <td>
                  <div>
                    <h1>Acknowledgements</h1>

                    This research was supported by the DARPA Machine Common Sense program, ONR MURI, as well as BAIR/BDD sponsors. We thank members of the BAIR community for helpful discussions. We also thank StabilityAI for their generous compute grant. This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>. Music credits: <a href="https://vye16.github.io/slahmr/">SLAHMR</a>
                  </div>
                </td>
              </tr>
            </table>

        <br><br>
</body>
</html>
