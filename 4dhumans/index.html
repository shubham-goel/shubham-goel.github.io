

<!-- Global site tag (gtag.js) - Google Analytics -->
<!-- <script async src="https://www.googletagmanager.com/gtag/js?id=UA-173306653-1"></script> -->
<!-- <script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-173306653-1');
</script> -->


<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
    body {
        font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
        font-weight:300;
        font-size:18px;
        margin-left: auto;
        margin-right: auto;
        width: 1100px;
    }

    h1 {
        font-weight:300;
    }

    .disclaimerbox {
        background-color: #eee;
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
        padding: 20px;
    }

    video.header-vid {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.header-img {
        height: 140px;
        border: 1px solid black;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    img.rounded {
        border: 1px solid #eeeeee;
        border-radius: 10px ;
        -moz-border-radius: 10px ;
        -webkit-border-radius: 10px ;
    }

    a:link,a:visited
    {
        color: #1367a7;
        text-decoration: none;
    }
    a:hover {
        color: #208799;
    }

    td.dl-link {
        height: 160px;
        text-align: center;
        font-size: 22px;
    }

    .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
                15px 15px 0 0px #fff, /* The fourth layer */
                15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
                20px 20px 0 0px #fff, /* The fifth layer */
                20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
                25px 25px 0 0px #fff, /* The fifth layer */
                25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
        margin-left: 10px;
        margin-right: 45px;
    }


    .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
        box-shadow:
                0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
                5px 5px 0 0px #fff, /* The second layer */
                5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
                10px 10px 0 0px #fff, /* The third layer */
                10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
        margin-top: 5px;
        margin-left: 10px;
        margin-right: 30px;
        margin-bottom: 5px;
    }

    .vert-cent {
        position: relative;
        top: 50%;
        transform: translateY(-50%);
    }

    hr
    {
        border: 0;
        height: 1.5px;
        background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
    }
</style>

<html>
  <head>
        <title>4D Humans</title>
        <meta property="og:title" content="4DH" />
        <!-- <meta property="og:image" content="https://shubham-goel.github.io/ucmr/resources/images/teaser.png" /> -->
        <!-- #TODO -->
        <!-- <meta property="og:url" content="https://www.youtube.com/watch?v=9-Ttb8jsevo" />  -->
  </head>

  <body>
    <br>
    <center>
    <span style="font-size:42px">Humans in 4D: <br> Reconstructing and Tracking Humans with Transformers</span>

    </center>

    <br><br>
      <table align=center width=1000px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~shubham-goel/">Shubham Goel</a></span>
        </center>
        </td>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://geopavlakos.github.io/">Georgios Pavlakos</a></span>
        </center>
        </td>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://people.eecs.berkeley.edu/~jathushan/">Jathushan Rajasegaran</a></span>
        </center>
        </td>
    <!-- </tr>
   </table>
   <table align=center width=800px>
    <tr> -->
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa <sup>*</sup> </a></span>
        </center>
        </td>

        <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="http://www.eecs.berkeley.edu/~malik/">Jitendra Malik <sup>*</sup> </a></span>
        </center>
        </td>
     </tr>
    </table>

    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">University of California, Berkeley</span>
                </center>
            </td>
        </tr>
    </table>

    <br>
    <table align=center width=700px>
        <tr>
            <td align=center width=100px>
                <center>
                    <span style="font-size:20px">arXiv preprint 2023</span>
                </center>
            </td>
        </tr>
    </table>


    <!-- <table align=center width=700px>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px">University of California, Berkeley<br/>Under Submission
            <br><br>
            Please do not share publicly
        </span>
        </center>
        </td>
     </tr>
       <tr>
        <td align=center width=100px>
        <center>
        <span style="font-size:20px"></span>
        </center>
        </td>
     </tr>
    </table> -->

    <br>
    <table align=center width=400px>
     <tr>
       <td align=center width=100px>
       <center>
       <span style="font-size:20px"><a href="https://arxiv.org/pdf/2305.20091.pdf">[Paper]</a></span>
       </center>
       </td>


       <!-- <td align=center width=100px>
        <center>
        <span style="font-size:20px"><a href="https://arxiv.org/abs/2007.10982">[Arxiv]</a></span>
        </center>
        </td> -->

      <!-- <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://youtu.be/9-Ttb8jsevo">[Video]</a></span>
      </center>
      </td> -->

      <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://github.com/shubham-goel/4D-Humans">[Code]</a></span>
      </center>
      </td>

      <td align=center width=100px>
      <center>
      <span style="font-size:20px"><a href="https://huggingface.co/spaces/brjathu/HMR2.0">[Online Demo]</a></span>
      </center>
      </td>

   </tr>
  </table>

            <br>
            <br>
            <table align=center width=1000px>
                <tr>
                    <td width=1000px>
                      <center>
                          <video loop controls muted autoplay width="1000px">
                            <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video1.mp4" type="video/mp4">
                            </video>
                    </center>
                    </td>
                </tr>
                    <td width=200px>
                      <center>
                          <span style="font-size:14px"><i>We present an approach that, given any video in the wild, can jointly reconstruct the underlying humans in 3D, and track them over time. 
                            <!-- At the core, is HMR 2.0, our robust model for recovering human meshes from single images.  -->
                            Here, we show the input video on the left, and the reconstructed humans on the right, wihtout any temporal smoothness, with colors indicating track identities over time. Our approach works reliably on usual and unusual poses, under poor visibility, extreme truncations and extreme occlusions.
                    </center>
                    </td>
                </tr>
            </table>

            <br><br>
            <!-- Bold abstract subtitle -->
            <h4>Abstract</h4>
            We present an approach to reconstruct humans and track them over time. At the core of our approach, we propose a fully "transformerized" version of a network for human mesh recovery. This network, HMR 2.0, advances the state of the art and shows the capability to analyze unusual poses that have in the past been difficult to reconstruct from single images. To analyze video, we use 3D reconstructions from HMR 2.0 as input to a tracking system that operates in 3D. This enables us to deal with multiple people and maintain identities through occlusion events. Our complete approach, 4DHumans, achieves state-of-the-art results for tracking people from monocular video. Furthermore, we demonstrate the effectiveness of HMR 2.0 on the downstream task of action recognition, achieving significant improvements over previous pose-based action recognition approaches.
            <br><br>

                <hr>

                <center><h1>Approach</h1></center>
        
                <table align=center width=1000px>
                        <tr>
                          <center>
                            <!-- <a href='https://github.com/shubham-goel/4D-Humans'> -->
                              <img class="round" style="height:400" src="./approach.png"/>
                          <!-- </a> -->
                          </center>
                      </tr>

                    <tr>
                      <td width=600px>
                        <center>
                            <span style="font-size:14px"><i>Left: HMR 2.0 is a fully "transformerized" version of a network for Human Mesh Recovery, containing a ViT and a cross-attention-based transformer decoder. 
                            Right: We use HMR 2.0 as the backbone of our 4DHumans system, that builds on <a href="https://people.eecs.berkeley.edu/~jathushan/PHALP/">PHALP</a>, to jointly reconstruct and track humans in 4D.</i>
                      </center>
                      </td>
                  </tr>
                  </table>
                    <br>
                  <hr>

                <center><h1>Results</h1></center>
                <!-- <center><h3>Reconstruction and tracking in the wild</h3></center>
                <table align=center width=1000px>
                  <tr>
                    <td width=1000px>
                      <center>
                        <div class = "video">
                          
                            <video loop controls width="1000px">
                            <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video1.mp4" type="video/mp4">
                            </video>

                            

                        </div>
                      </center>
                    </td>
                  </tr>

                    <tr>
                        <td width=600px>
                          <center>
                              <span style="font-size:14px"><i>Human reconstruction and tracking results on videos in the wild. Music credits: <a href="https://vye16.github.io/slahmr/">SLAHMR</a>.</i>
                        </center>
                        </td>
                    </tr>
                </table>
                <br> -->

                <center><h4>Comparisons to existing approaches</h4></center>
                <table align=center width=900px>
                  <tr>
                    <td width=900px>
                      <center>
                        <div class = "video">
                          
                            <video loop controls muted autoplay width="900px">
                            <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video2_comp.mp4" type="video/mp4">
                            </video>

                            

                        </div>
                      </center>
                    </td>
                  </tr>

                    <tr>
                        <td width=600px>
                          <center>
                              <span style="font-size:14px"><i>We present comparisons to previous state-of-the-art approaches (PyMAF-X and PARE) for lifting humans to 3D. HMR 2.0 reconstructions are temporally stable, and are better aligned to the input video. The baselines have a lot of jitter and sometimes even fail completely on hard poses or unusual viewpoints.</i>
                        </center>
                        </td>
                    </tr>
                </table>
                <br>

                <center><h4>Side-View Visualizations</h4></center>
                <table align=center width=900px>
                  <tr>
                    <td width=900px>
                      <center>
                        <div class = "video">
                          
                                <video loop controls muted autoplay width="700px">
                                <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video3_side.mp4" type="video/mp4">
                                </video>

                        </div>
                    </center>
                    </td>
                  </tr>
                  <tr>
                      <td width=600px>
                        <center>
                            <span style="font-size:14px"><i>Here, we visualize the reocnstructed mesh from a novel view, in addition to the camera view. HMR 2.0 reconstructions are plausible and temporally stable, even when observed from a novel view, despite it being per-frame 3D reconstruction without temporal smoothness.</i>
                      </center>
                      </td>
                  </tr>
                </table>
                <br>

                <!-- <center><h3>Amodal Completion</h3></center>
                <table align=center width=900px>
                  <tr>
                    <td width=900px>
                      <center>
                        <div class = "video">
                          
                                <video loop controls width="900px">
                                <source src="https://people.eecs.berkeley.edu/~shubham-goel/projects/4DHumans/resources/videos/video4_amodal.mp4" type="video/mp4">
                                </video>

                        </div>
                    </center>
                    </td>
                  </tr>
                  <tr>
                      <td width=600px>
                        <center>
                            <span style="font-size:14px"><i>We can amodally complete our tracklets in frames with missing detections (e.g., occlusions)</i>
                      </center>
                      </td>
                  </tr>
                </table>
                <br> -->
                <hr>



            <table align=center width=1100px>
                <tr>
                    <td>
                      <left>
                <center><h1>Acknowledgements</h1></center>
                This research was supported by the DARPA Machine Common Sense program, ONR MURI, as well as BAIR/BDD sponsors. We thank members of the BAIR community for helpful discussions. We also thank StabilityAI for their generous compute grant. This webpage template was borrowed from some <a href="https://richzhang.github.io/colorization/">colorful folks</a>. Music credits: <a href="https://vye16.github.io/slahmr/">SLAHMR</a>
            </left>
        </td>
        </tr>
        </table>

        <br><br>
</body>
</html>
